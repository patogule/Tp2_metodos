{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Yc_QGcPrvhgj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploramos los datos\n",
        " Exploramos los datos para reconocer su estructura y tamaños necesarios de las x, w1,b1,w2,b2 e y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[2012.9166667, 2012.9166667, 2013.5833333, 2013.5, 2012.8333333, 2012.6666667, 2012.6666667, 2013.4166667, 2013.5, 2013.4166667], [32.0, 19.5, 13.3, 13.3, 5.0, 7.1, 34.5, 20.3, 31.7, 17.9], [84.87882, 306.5947, 561.9845, 561.9845, 390.5684, 2175.03, 623.4731, 287.6025, 5512.038, 1783.18], [10.0, 9.0, 5.0, 5.0, 5.0, 3.0, 7.0, 6.0, 1.0, 3.0], [24.98298, 24.98034, 24.98746, 24.98746, 24.97937, 24.96305, 24.97933, 24.98042, 24.95095, 24.96731], [121.54024, 121.53951, 121.54391, 121.54391, 121.54245, 121.51254, 121.53642, 121.54228, 121.48458, 121.51486]]\n",
            "[[0.27272729256195555, 0.9996688704673731, 1.0, 1.0, 0.9996688567112925, 0.9995860709263692, 0.9995860709263692, 0.9999586071075384, 1.0, 1.0], [0.9152542372881356, 0.5533686601059804, 0.37549031632232094, 0.37874565242060276, 0.13551871445319721, 0.20266510725642645, 1.0, 0.6388345527724546, 1.0, 1.0], [0.0, 0.05562274788381358, 0.10195584645824286, 0.10195584645824286, 0.07085734895151304, 0.39459633623715956, 0.11311117593891772, 0.05217716205875214, 1.0, 1.0], [1.0, 1.0, 0.6666666666666666, 0.6842105263157895, 0.6842105263157895, 0.368421052631579, 1.0, 1.0, 0.23999999999999996, 1.0], [0.8772938920844318, 0.9997046888865001, 1.0, 1.0, 0.9999564371859775, 0.9992793465908849, 0.9999547776502054, 1.0, 0.9993208804873058, 1.0], [0.9381425922804745, 0.9999635174992493, 1.0, 1.0, 1.0, 0.9997534081281186, 0.9999514112855002, 1.0, 0.999748873574841, 1.0]]\n"
          ]
        }
      ],
      "source": [
        "df_test= pd.read_excel('Real estate valuation data set.xlsx',  nrows=10)\n",
        "\n",
        "x = df_test.iloc[:, 1:7].values.transpose().tolist()\n",
        "\n",
        "\n",
        "for line in x:\n",
        "    for i in range(len(line)):\n",
        "        line[i] = (line[i] -min(line))/(max(line)-min(line))\n",
        "\n",
        "\n",
        "x = np.array(x)\n",
        "y = df_test.iloc[:, 7].values.tolist()\n",
        "\n",
        "\n",
        "print(x)\n",
        "print(y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Declaramos las funciones auxiliares"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "Zz81ZoDxvhgm"
      },
      "outputs": [],
      "source": [
        "def forward(W1, W2, b1, b2, x):\n",
        "   \n",
        "    f1 = np.dot(W1, x) + b1\n",
        "    sigma = 1 / (1 + np.exp(-f1))\n",
        "    f = np.dot(W2, sigma) + b2\n",
        "\n",
        "    return f\n",
        "\n",
        "def funcion_objetivo(x, y, W1, W2, b1, b2):\n",
        "\n",
        "\n",
        "    loss = 0.5 * (np.power(forward(W1, W2, b1, b2, x) - y, 2)) \n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "P-R0FjEzvhgn"
      },
      "outputs": [],
      "source": [
        "#Calculo del gradiente numerico\n",
        "\n",
        "def funcion_objetivo(x, y, W1, W2, b1, b2):\n",
        "    loss = 0.5 * (np.power(forward(W1, W2, b1, b2, x) - y, 2)) \n",
        "    return loss\n",
        "\n",
        "def numerical_gradient(W1, W2, b1, b2, x, y, epsilon):\n",
        "    #Tenemos que calcular el gradiente de la función objetivo en un punto, que equivale a computar las siguientes derivadas parciales respecto a cada elemento de las matrices W y los vectores b.\n",
        "\n",
        "    der_W1 = (funcion_objetivo(x, y, W1 + epsilon, W2, b1, b2) - funcion_objetivo(x, y, W1 - epsilon, W2, b1, b2)) / (2 * epsilon)\n",
        "    der_W2 = (funcion_objetivo(x, y, W1, W2 + epsilon, b1, b2) - funcion_objetivo(x, y, W1, W2 - epsilon, b1, b2)) / (2 * epsilon)\n",
        "    der_b1 = (funcion_objetivo(x, y, W1, W2, b1 + epsilon, b2) - funcion_objetivo(x, y, W1, W2, b1 - epsilon, b2)) / (2 * epsilon)\n",
        "    der_b2 = (funcion_objetivo(x, y, W1, W2, b1, b2 + epsilon) - funcion_objetivo(x, y, W1, W2, b1, b2 - epsilon)) / (2 * epsilon)  \n",
        "\n",
        "\n",
        "    return der_W1, der_W2, der_b1, der_b2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "rJRtIYxMvhgo"
      },
      "outputs": [],
      "source": [
        "#funcion fit y loop de entrenamiento\n",
        "#Implementar el método fit() que realiza el ciclo de entrenamiento de la red. En cada iteración, se calcula el valor del gradiente promedio para todas las muestras del dataset y se actualizan los parámetros de la función utilizando esta dirección.\n",
        "\n",
        "def fit(x, y, learning_rate=0.001, epochs=1000, eps=1e-6):\n",
        "    #Inicializacion de pesos\n",
        "    W1 = np.random.random((5,6))\n",
        "    b1 = np.random.random((5,1))\n",
        "    W2 = np.random.random((1,5))\n",
        "    b2 = np.random.random((1,1))\n",
        "\n",
        "    loss_accum = []\n",
        "    for iteracion in range(epochs):\n",
        "        der_W1, der_W2, der_b1, der_b2 = numerical_gradient(W1, W2, b1, b2, x, y, eps)\n",
        "        #Una vez que calculamos todos estos gradientes, podemos usarlos para actualizar la red. Hacemos esto restando una pequeña cantidad de cada derivada parcial al parámetro correspondiente. Esta cantidad es controlada por un parámetro llamado learning rate o tasa de aprendizaje.\n",
        "        #Es decir, para nuestro espacio de parámetros θ, calculamos:\n",
        "        W1 = W1 - learning_rate *np.mean(der_W1)\n",
        "        W2 = W2 - learning_rate * np.mean(der_W2)\n",
        "        b1 = b1 - learning_rate *np.mean( der_b1)\n",
        "        b2 = b2 - learning_rate *np.mean( der_b2)\n",
        "        \n",
        "        loss_accum.append(np.mean(funcion_objetivo(x, y, W1, W2, b1, b2)))\n",
        "\n",
        "    return loss_accum, W1, W2, b1, b2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "IS4sKd_Hvhgo"
      },
      "outputs": [],
      "source": [
        "def predict(x,W1, W2, b1, b2):\n",
        "        y = forward(W1, W2, b1, b2, x)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[93.20836609512683, 93.20755148814942, 93.20673843803029, 93.20592694027043, 93.20511699038809, 93.20430858391879, 93.2035017164153, 93.20269638344742, 93.20189258060198, 93.2010903034828]\n",
            "\n",
            "maximo:  650.4880078817973\n",
            "minimo:  93.2010903034828\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x223f98d7880>]"
            ]
          },
          "execution_count": 173,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "df_test= pd.read_excel('Real estate valuation data set.xlsx',  nrows=10000)\n",
        "\n",
        "x = df_test.iloc[:, 1:7].values.transpose().tolist()\n",
        "mean = lambda x: sum(x) / len(x)\n",
        "std = lambda x: (sum([(i - mean(x))**2 for i in x]) / len(x))**0.5\n",
        "\n",
        "for line in x:\n",
        "    for i in range(len(line)):\n",
        "\n",
        "        line[i] = (line[i] -mean(line))/(std(line))\n",
        "\n",
        "\n",
        "\n",
        "x = np.array(x)\n",
        "\n",
        "y = df_test.iloc[:, 7].values.tolist()\n",
        "\n",
        "\n",
        "loss, W1, W2, b1, b2 = fit(x, y, learning_rate=0.001, epochs=1000,eps=0.001)\n",
        "print(loss[-10:])\n",
        "print()\n",
        "print('maximo: ', max(loss))\n",
        "print('minimo: ', min(loss))\n",
        "print()\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss vs Epochs')\n",
        "plt.plot(range(len(loss)),loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como podemos ver los datos convergen rapidamente, pero llegamos a un mínimo local probablemente ya que no podemos disminuir el loss value mas alla del 93. Esto se puede deber a que el modelo no es lo suficientemente complejo para representar la función que genera los datos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pruebo predecir con los datos de entrenamiento los datos de prueba"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Entrenamos la red neuronal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train= pd.read_excel('Real estate valuation data set.xlsx',  nrows=315)\n",
        "#declaro y normalizo las x\n",
        "x = df_train.iloc[:, 1:7].values.transpose().tolist()\n",
        "mean = lambda x: sum(x) / len(x)\n",
        "std = lambda x: (sum([(i - mean(x))**2 for i in x]) / len(x))**0.5\n",
        "for line in x:\n",
        "    for i in range(len(line)):\n",
        "        line[i] = (line[i] -mean(line))/(std(line))\n",
        "x = np.array(x)\n",
        "#declaro las y\n",
        "y = df_train.iloc[:, 7].values.tolist()\n",
        "loss, W1, W2, b1, b2 = fit(x, y, learning_rate=0.001, epochs=2000,eps=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluamos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5051009498235535\n"
          ]
        }
      ],
      "source": [
        "df_prueba = pd.read_excel('Real estate valuation data set.xlsx',  nrows=414, skiprows=315)\n",
        "\n",
        "#declaro y normalizo las x\n",
        "x = df_prueba.iloc[:, 1:7].values.transpose().tolist()\n",
        "mean = lambda x: sum(x) / len(x)\n",
        "std = lambda x: (sum([(i - mean(x))**2 for i in x]) / len(x))**0.5\n",
        "for line in x:\n",
        "    for i in range(len(line)):\n",
        "        line[i] = (line[i] -mean(line))/(std(line))\n",
        "x = np.array(x)\n",
        "#declaro las y\n",
        "y =np.array( df_prueba.iloc[:, 7].values.tolist())\n",
        "\n",
        "pred = predict(x,W1, W2, b1, b2)\n",
        "print(abs(np.mean(y-pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "como vemos la diferencia promedio es muy baja, por lo que podemos decir que la red neuronal aprendió a predecir los datos de prueba"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Buscamos los hipérparametros óptimos para el modelo de regresión\n",
        "Realizamos un muestreo de parametros y comparamos sus loss values para encontrar los mejores parametros para el modelo de regresión. Guardamos los resultados en un archivo csv."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Funcion que itera los hiperparametros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(eps_range, learning_rate_range, epochs_range,x, y):\n",
        "    training_list = []\n",
        "    #itera todas las combinaciones de eps, learning rate y epochs\n",
        "    for eps in range(eps_range[0], eps_range[1]):\n",
        "        for learning_rate in range(learning_rate_range[0], learning_rate_range[1]):\n",
        "            for epochs in range(epochs_range[0], epochs_range[1]):\n",
        "                loss, W1, W2, b1, b2 = fit(x, y, learning_rate=1*(10**-learning_rate), epochs=10**epochs,eps=10**-eps)\n",
        "                training_list.append([loss[-1], epochs, learning_rate, eps])\n",
        "    return training_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ejecuto la funcion con distintos rangos de hiperparametros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_test= pd.read_excel('Real estate valuation data set.xlsx',  nrows=10000)\n",
        "\n",
        "#declaro y normalizo las x\n",
        "x = df_test.iloc[:, 1:7].values.transpose().tolist()\n",
        "mean = lambda x: sum(x) / len(x)\n",
        "std = lambda x: (sum([(i - mean(x))**2 for i in x]) / len(x))**0.5\n",
        "for line in x:\n",
        "    for i in range(len(line)):\n",
        "\n",
        "        line[i] = (line[i] -mean(line))/(std(line))\n",
        "x = np.array(x)\n",
        "\n",
        "#declaro las y\n",
        "y = df_test.iloc[:, 7].values.tolist()\n",
        "\n",
        "\n",
        "training_list = train_model([2, 6], [2, 6], [2, 5], x, y)\n",
        "losses = [x[0] for x in training_list]\n",
        "\n",
        "pd.DataFrame(training_list).to_csv('training.csv',sep=';', index=False, header=['loss', 'epochs', 'learning_rate', 'eps'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         loss  epochs  learning_rate      eps\n",
            "17  78.238050   10000          0.001  0.00100\n",
            "5   91.587170   10000          0.001  0.01000\n",
            "14  92.351984   10000          0.010  0.00100\n",
            "38  92.354608   10000          0.010  0.00001\n",
            "26  92.354637   10000          0.010  0.00010\n",
            "41  92.441461   10000          0.001  0.00001\n",
            "25  92.442266    1000          0.010  0.00010\n",
            "1   92.442285    1000          0.010  0.01000\n",
            "29  92.442779   10000          0.001  0.00010\n",
            "37  92.442789    1000          0.010  0.00001\n"
          ]
        }
      ],
      "source": [
        "csv = pd.read_csv('training.csv', sep=';')\n",
        "csv = csv.sort_values(by=['loss'])\n",
        "print(csv[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusiones\n",
        "En los hiperparametros podemos observar que la gran mayoria de las buenas aproximacion tienen un learning rate de 0.001 y un batch size de 10000. Por lo que podemos concluir que estos son los mejores hiperparametros para el modelo de regresion. el epsilon para learning_rates mas grandes es mas chico, probablemente porque entre ellos si regulan para llegar a un ratio optimo. Respecto al batch, si bien para mayor cantidad de datos la estimacion mejora, sobre todo para learning rates muy bajos, no parece ser muy significativa la diferencia a partir de 1000 datos en adelante. \n",
        "Por lo que podriamos concluir que el learning rate es el hiperparametro mas importante para el modelo de regresion, o que por lo menos regula a los otros dos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Entonces, elegimos como hiperparametros para el modelo de regresion: learning rate = 0.001 y epochs = 10000 y epsilon = 0.001"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
