{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yc_QGcPrvhgj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sympy as sp\n",
        "from sympy import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zz81ZoDxvhgm"
      },
      "outputs": [],
      "source": [
        "def forward(W1, W2, b1, b2, x):\n",
        "   \n",
        "    f1 = np.dot(x, W1) + b1\n",
        "    sigma = 1 / (1 + np.exp(-f1))\n",
        "    f = np.dot(sigma, W2) + b2\n",
        "\n",
        "    return f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpp8zX0evhgn"
      },
      "outputs": [],
      "source": [
        "#Inicializacion de pesos\n",
        "W1 = np.random.random((5,6))\n",
        "b1 = np.random.random((5,1))\n",
        "\n",
        "W2 = np.random.random((1,5))\n",
        "b2 = np.random.random((1,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-R0FjEzvhgn"
      },
      "outputs": [],
      "source": [
        "#Calculo del gradiente numerico\n",
        "\n",
        "def funcion_objetivo(x, y, W1, W2, b1, b2):\n",
        "    loss = 0.5 * (np.power(forward(W1, W2, b1, b2, x) - y, 2)) \n",
        "    return loss\n",
        "\n",
        "def numerical_gradient(W1, W2, b1, b2, x, y, epsilon, alfa):\n",
        "    #Tenemos que calcular el gradiente de la función objetivo en un punto, que equivale a computar las siguientes derivadas parciales respecto a cada elemento de las matrices W y los vectores b.\n",
        "\n",
        "    der_W1 = (funcion_objetivo(x, y, W1 + epsilon, W2, b1, b2) - funcion_objetivo(x, y, W1 - epsilon, W2, b1, b2)) / (2 * epsilon)\n",
        "    der_W2 = (funcion_objetivo(x, y, W1, W2 + epsilon, b1, b2) - funcion_objetivo(x, y, W1, W2 - epsilon, b1, b2)) / (2 * epsilon)\n",
        "    der_b1 = (funcion_objetivo(x, y, W1, W2, b1 + epsilon, b2) - funcion_objetivo(x, y, W1, W2, b1 - epsilon, b2)) / (2 * epsilon)\n",
        "    der_b2 = (funcion_objetivo(x, y, W1, W2, b1, b2 + epsilon) - funcion_objetivo(x, y, W1, W2, b1, b2 - epsilon)) / (2 * epsilon)  \n",
        "\n",
        "    #Una vez que calculamos todos estos gradientes, podemos usarlos para actualizar la red. Hacemos esto restando una pequeña cantidad de cada derivada parcial al parámetro correspondiente. Esta cantidad es controlada por un parámetro llamado learning rate o tasa de aprendizaje. Es decir, para nuestro espacio de parámetros θ, calculamos:\n",
        "    W1_nuevo = W1 - alfa * der_W1\n",
        "    W2_nuevo = W2 - alfa * der_W2\n",
        "    b1_nuevo = b1 - alfa * der_b1\n",
        "    b2_nuevo = b2 - alfa * der_b2\n",
        "\n",
        "    #guardamos los nuevos parametros en un arreglo llamado gradiente\n",
        "    gradiente = [W1_nuevo, W2_nuevo, b1_nuevo, b2_nuevo]\n",
        "\n",
        "    return gradiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJRtIYxMvhgo"
      },
      "outputs": [],
      "source": [
        "#funcion fit y loop de entrenamiento\n",
        "#Implementar el método fit() que realiza el ciclo de entrenamiento de la red. En cada iteración, se calcula el valor del gradiente promedio para todas las muestras del dataset y se actualizan los parámetros de la función utilizando esta dirección.\n",
        "\n",
        "def fit(x, y, learning_rate=0.001, epochs=1000):\n",
        "    eps = 1e-3\n",
        "    loss_accum = []\n",
        "    for iteracion in range(epochs):\n",
        "        \n",
        "        W1, W2, b1, b2 = numerical_gradient(W1, W2, b1, b2, x, y, eps, learning_rate)\n",
        "        loss_accum.append(np.mean(funcion_objetivo(x, y, W1, W2, b1, b2)))\n",
        "\n",
        "    return loss_accum, W1, W2, b1, b2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IS4sKd_Hvhgo"
      },
      "outputs": [],
      "source": [
        "def predict(x):\n",
        "        y = forward(W1, W2, b1, b2, x)\n",
        "        return y"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
